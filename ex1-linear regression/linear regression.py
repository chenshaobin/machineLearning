#!/usr/bin/python
# -*- coding: utf-8 -*-
import pandas as pd
import seaborn as sns
sns.set(context="notebook", style="whitegrid", palette="dark")
import matplotlib.pyplot as plt
# import tensorflow as tf
import numpy as np

data = pd.read_csv('ex1data1.txt', names=['population', 'profit'])
# data.head()
# data.info()
sns.lmplot('population', 'profit', data, fit_reg=False)
#plt.show()
"""
    # å¤šå˜é‡çš„å‡è®¾ h è¡¨ç¤ºä¸ºï¼š
    # â„ğœƒ(ğ‘¥)=ğœƒ0+ğœƒ1ğ‘¥1+ğœƒ2ğ‘¥2+...+ğœƒğ‘›ğ‘¥ğ‘›
    # è¿™ä¸ªå…¬å¼ä¸­æœ‰n+1ä¸ªå‚æ•°å’Œnä¸ªå˜é‡ï¼Œä¸ºäº†ä½¿å¾—å…¬å¼èƒ½å¤Ÿç®€åŒ–ä¸€äº›ï¼Œå¼•å…¥ ğ‘¥0=1 ï¼Œåˆ™å…¬å¼è½¬åŒ–ä¸ºï¼š
    # æ­¤æ—¶æ¨¡å‹ä¸­çš„å‚æ•°æ˜¯ä¸€ä¸ªn+1ç»´çš„å‘é‡ï¼Œä»»ä½•ä¸€ä¸ªè®­ç»ƒå®ä¾‹ä¹Ÿéƒ½æ˜¯n+1ç»´çš„å‘é‡ï¼Œç‰¹å¾çŸ©é˜µXçš„ç»´åº¦æ˜¯ m*(n+1),mä¸ºæ•°æ®ä¸ªæ•°
    # å› æ­¤å…¬å¼å¯ä»¥ç®€åŒ–ä¸ºï¼š â„ğœƒ(ğ‘¥)=ğœƒğ‘‡ğ‘‹ ï¼Œå…¶ä¸­ä¸Šæ ‡Tä»£è¡¨çŸ©é˜µè½¬ç½®ã€‚
"""


def get_X(df):
    # è¯»å–ç‰¹å¾population
    ones = pd.DataFrame({'ones': np.ones(len(df))})  # onesæ˜¯mè¡Œ1åˆ—çš„dataframe
    data = pd.concat([ones, df], axis=1)    # æ ¹æ®åˆ—åˆå¹¶æ•°æ®
    return data.iloc[:, :-1].values     # è¿”å›ndarray,ä¸æ˜¯çŸ©é˜µ


def get_y(df):
    # è¯»å–æ ‡ç­¾å€¼
    return np.array(df.iloc[:, -1])     # dfçš„æœ€åä¸€åˆ—


# print(get_y(data))
X = get_X(data)
y = get_y(data)
# print(y.shape, type(y))


def normalize_feature(df):
    # ç‰¹å¾ç¼©æ”¾æ“ä½œï¼Œå¯¹æ•°æ®è¿›è¡Œå‡å€¼å½’ä¸€åŒ–
    return df.apply(lambda column: (column - column.mean()) / column.std())


"""
    # ä»£ä»·å‡½æ•°çš„è®¡ç®—
    # ğ½(ğœƒ)=1/2ğ‘š * âˆ‘ğ‘–=1ğ‘š(â„ğœƒ(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))2
    # å…¶ä¸­ï¼šâ„ğœƒ(ğ‘¥)=ğœƒğ‘‡ğ‘‹=ğœƒ0ğ‘¥0+ğœƒ1ğ‘¥1+ğœƒ2ğ‘¥2+...+ğœƒğ‘›ğ‘¥ğ‘›
"""

# å‚æ•°å‘é‡ğœƒ
theta = np.zeros(X.shape[1])  # X.shape[1]=2,ä»£è¡¨ç‰¹å¾æ•°n


def lr_cost(theta, X, y):
    """

    :param theta: å‚æ•°å‘é‡ğœƒ
    :param X: R(m*n), m æ ·æœ¬æ•°, n ç‰¹å¾æ•°
    :param y: R(m),profit
    :return: ä»£ä»·å‡½æ•°å€¼
    """
    m = X.shape[0]   # æ ·æœ¬æ•°
    inner = X @ theta - y       # çŸ©é˜µä¹˜ä»¥å‘é‡ï¼Œç­‰ä»·äºX.dot(theta),çŸ©é˜µç›¸ä¹˜
    """
        # 1*m @ m*1 = 1*1 in matrix multiplication
        # but you know numpy didn't do transpose in 1d array, so here is just a
        # vector inner product to itselves
    """
    square_sum = inner.T @ inner
    cost = square_sum / (2 * m)
    return cost


# print('cost:', lr_cost(theta, X, y))
# è®¡ç®—ä»£ä»·å‡½æ•°çš„å¯¼æ•°å€¼

def gradient(theta, X, y):
    m = X.shape[0]
    inner = X.T @ (X @ theta - y)     # (m,n).T @ (m, 1) -> (n, 1)ï¼ŒX @ thetaç­‰ä»·äºX.dot(theta)
    return inner / m

"""
    # batch gradient decentï¼ˆæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼‰
    # ğœƒğ‘—:=ğœƒğ‘—âˆ’ğ›¼âˆ‚âˆ‚ğœƒğ‘—ğ½(ğœƒ)
"""
def batch_gradient_decent(theta, X, y, epoch, alpha = 0.01):
    """
    :param theta:
    :param X:
    :param y:
    :param epoch: æ‰¹å¤„ç†çš„è½®æ•°
    :param alpha:
    :return:
    """
    cost_data = [lr_cost(theta, X, y)]      # æ¯”è¾ƒéšç€è¿­ä»£æ¬¡æ•°çš„å¢å¤šï¼Œæ¯”è¾ƒä»£ä»·å‡½æ•°å€¼çš„å˜åŒ–
    _theta = theta.copy()
    for i in range(epoch):
        _theta = _theta - alpha * gradient(_theta, X, y)
        cost_data.append(lr_cost(_theta, X, y))

    return _theta, cost_data

epoch = 600
final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch)

# print('final_theta', final_theta)
# print('cost_data', cost_data)
# print('æœ€ç»ˆçš„ä»£ä»·å‡½æ•°å€¼ï¼š', lr_cost(final_theta, X, y))

# å°†ä»£ä»·æ•°æ®å¯è§†åŒ–,æ¯”è¾ƒéšç€è¿­ä»£æ¬¡æ•°çš„å¢å¤šï¼Œæ¯”è¾ƒä»£ä»·å‡½æ•°å€¼çš„å˜åŒ–
"""
ax = sns.tsplot(cost_data, time=np.arange(epoch+1))     # æ–°ç‰ˆæœ¬æ²¡æœ‰tsplotï¼ˆï¼‰
ax.set_xlabel('epoch')
ax.set_ylabel('cost')
plt.show()
"""

"""
b = final_theta[0]
m = final_theta[1]
plt.scatter(data.population, data.profit, label="Training data")
plt.plot(data.population, data.population*m + b, label="Prediction")    # ç”»å‡ºå‡è®¾å‡½æ•°
plt.legend(loc=2)   # å·¦ä¸Šè§’
plt.show()
"""
