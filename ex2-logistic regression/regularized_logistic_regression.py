#!/usr/bin/python
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')        #This shows an example of the "fivethirtyeight" styling, which tries to replicate the styles from FiveThirtyEight.com.
import scipy.optimize as opt     # åˆ©ç”¨å…¶ä¸­çš„opt.minimizeæ–¹æ³•å¯»æ‰¾æœ€å°å‚æ•°
from sklearn.metrics import classification_report  # è¿™ä¸ªåŒ…æ˜¯è¯„ä»·æŠ¥å‘Š

df = pd.read_csv('ex2data2.txt', names=['test1', 'test2', 'accepted'])
# print(df)
# print(df.head())

# ç”»å‡ºæ•°æ®åˆ†å¸ƒ
"""
sns.set(context="notebook", style="ticks", font_scale=1.5)

sns.lmplot('test1', 'test2', hue='accepted', data=df, height=6, fit_reg=False, scatter_kws={"s": 50})
plt.title('Regularized Logistic Regression')
plt.show()
"""
# ç”»å‡ºæ•°æ®åˆ†å¸ƒend
def get_y(df):
    # è¯»å–æ ‡ç­¾å€¼
    return np.array(df.iloc[:, -1])     # dfçš„æœ€åä¸€åˆ—

# æ„å»ºä¸€ä¸ªå¸¸ç”¨çš„é€»è¾‘å‡½æ•°ï¼ˆlogistic functionï¼‰ä¸ºSå½¢å‡½æ•°ï¼ˆSigmoid functionï¼‰
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# è®¡ç®—ä»£ä»·å‡½æ•°
def cost(theta, X, y):
    return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))   # X @ thetaä¸X.dot(theta)ç­‰ä»·

# æ¢¯åº¦ä¸‹é™ï¼Œè¿›è¡Œå‘é‡åŒ–è®¡ç®—,è®¡ç®—å¯¼æ•°å€¼
def gradient(theta, X, y):
    return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)

# è®­ç»ƒå€¼é¢„æµ‹å’Œåˆ†æ
def predict(x, theta):
    prob = sigmoid(x @ theta)
    return (prob >= 0.5).astype(int)    # >=0.5åˆ™è¡¨ç¤ºé¢„æµ‹æ­£ç¡®

"""
    # å¦‚æœæ ·æœ¬é‡å¤šï¼Œé€»è¾‘å›å½’é—®é¢˜å¾ˆå¤æ‚ï¼Œè€ŒåŸå§‹ç‰¹å¾åªæœ‰x1,x2å¯ä»¥ç”¨å¤šé¡¹å¼åˆ›å»ºæ›´å¤šçš„ç‰¹å¾x1ã€x2ã€x1x2ã€x1^2ã€x2^2ã€... X1^nX2^nã€‚
    # å› ä¸ºæ›´å¤šçš„ç‰¹å¾è¿›è¡Œé€»è¾‘å›å½’æ—¶ï¼Œå¾—åˆ°çš„åˆ†å‰²çº¿å¯ä»¥æ˜¯ä»»æ„é«˜é˜¶å‡½æ•°çš„å½¢çŠ¶ã€‚
"""
# ç‰¹å¾æ˜ å°„å‡½æ•°
def feature_mapping(x1, x2, power, as_ndarray_flag=False):
    """
        # x1, x2 ä¸ºæ•°ç»„
        # return mapped features as ndarray or dataframe
        # data = {}
        # # inclusive
        # for i in np.arange(power + 1):
        #     for p in np.arange(i + 1):
        #         data["f{}{}".format(i - p, p)] = np.power(x, i - p) * np.power(y, p)
    """
    data = {"f{}{}".format(i-p, p): np.power(x1, i - p) * np.power(x2, p) for i in np.arange(power+1) for p in np.arange(i+1)}
    if as_ndarray_flag:
        return pd.DataFrame(data).values    # æ•°ç»„å½¢å¼
    else:
        return pd.DataFrame(data)   # è¿˜æ˜¯DataFrameæ ¼å¼

x1 = np.array(df.test1)
# print(x1)
x2 = np.array(df.test2)
data = feature_mapping(x1, x2, power=6)     # ç»è¿‡ç‰¹è¯Šæ˜ å°„åçš„ç‰¹å¾æ•°æ®
# print(data.shape)
# print(data.head())
# print(data.describe())

theta = np.zeros(data.shape[1])    # n*1çš„ndarrayæ•°ç»„,ä¸€ç»´
X = feature_mapping(x1, x2, power=6, as_ndarray_flag=True)   # è·å–æ•°æ®ï¼Œæ•°ç»„å½¢å¼
# print(X.shape)
y = get_y(df)
# print(y.shape)
# è®¡ç®—æ­£åˆ™åŒ–ä»£ä»·å‡½æ•°
def regularized_cost(theta, X, y, l=1):
    # do not penalize theta_0
    theta_1_to_n = theta[1:]
    regularized_term = (l / (2 * len(X))) * np.power(theta_1_to_n, 2).sum()
    return cost(theta, X, y) + regularized_term

# æµ‹è¯•æ­£åˆ™åŒ–ä»£ä»·å‡½æ•°
# print('init cost = {}'.format(regularized_cost(theta, X, y)))

# è®¡ç®—æ­£åˆ™åŒ–æ¢¯åº¦
def regularized_gradient(theta, X, y, l =1):
    # do not penalize theta_0
    theta_1_to_n = theta[1:]
    regularized_theta = (l / len(X)) * theta_1_to_n
    regularized_term = np.concatenate([np.array([0]), regularized_theta])
    return gradient(theta, X, y) + regularized_term

# æµ‹è¯•æ­£åˆ™åŒ–æ¢¯åº¦
# print(regularized_gradient(theta, X, y))

# å‚æ•°æ‹Ÿåˆ
res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y), method='Newton-CG', jac=regularized_gradient)
# print(res)
final_theta = res.x
y_pred = predict(X, final_theta)
analysis_result_report = classification_report(y, y_pred, target_names=['test1', 'test2'])
# print(analysis_result_report)


def regularized_logistic_regression(power, l):
    """
        # å°†æ•°æ®è¯»å–ã€ç‰¹å¾æ˜ å°„ã€åˆ©ç”¨æ¢¯åº¦ä¸‹é™è®¡ç®—æœ€ä¼˜å‚æ•°ç»Ÿåˆåœ¨ä¸€ä¸ªå‡½æ•°å½“ä¸­
        #   power: int
        #   l: int
        #   returnï¼šfinal_theta
    """

    df = pd.read_csv('ex2data2.txt', names=['test1', 'test2', 'accepted'])
    x1 = np.array(df.test1)
    x2 = np.array(df.test2)
    y = get_y(df)
    X = feature_mapping(x1, x2, power=6, as_ndarray_flag=True)  # è·å–æ•°æ®ï¼Œæ•°ç»„å½¢å¼
    theta = np.zeros(data.shape[1])  # n*1çš„ndarrayæ•°ç»„,ä¸€ç»´
    # å°è¯•å…¶å®ƒmethodæ–¹æ³•,æ„Ÿè§‰åŒºåˆ«ä¸æ˜¯ç‰¹åˆ«å¤§; argså‚æ•°ä¸­åº”è¯¥å¸¦lï¼Œ
    res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y, l), method='TNC', jac=regularized_gradient)
    final_theta  = res.x
    return final_theta

def find_decision_boundary(density, power, theta, threshold):
    """
    # æ‰¾åˆ°æ‰€æœ‰æ»¡è¶³  ğ‘‹Ã—ğœƒ=0  çš„x
    # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿå¯†é›†çš„xã€yç½‘æ ¼ï¼Œåˆ©ç”¨å‚æ•°thetaï¼Œæ‰¾åˆ°ğ‘‹Ã—ğœƒè¶³å¤Ÿå°äº0çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å…¶ä¸­çš„ä¸¤ç»„æ•°æ®ä½œä¸ºå†³ç­–è¾¹ç•Œå‡½æ•°çš„x,y

    :param density: å†³å®šxã€yå–å€¼çš„å¯†é›†åº¦
    :param power: å†³å®šå¤šé¡¹å¼çš„å¹‚
    :param theta: å‚æ•°
    :param threshold: é˜ˆå€¼è®¾ç½®
    :return: ç”¨äºç”»å‡ºå†³ç­–è¾¹ç•Œçš„xã€y
    """
    t1 = np.linspace(-1, 1.5, density)
    t2 = np.linspace(-1, 1.5, density)
    cordinates = [(x, y) for x in t1 for y in t2]
    x_cord, y_cord = zip(*cordinates)
    mapped_cord = feature_mapping(x_cord, y_cord, power)    # return a dataFrame
    inner_product = mapped_cord.values @ theta
    decision = mapped_cord[np.abs(inner_product) < threshold]       # æ‰¾åˆ°ğ‘‹Ã—ğœƒè¶³å¤Ÿå°äº0çš„æ˜ å°„ç‰¹å¾æ•°æ®,è¿™é‡Œçš„æ•°æ®æå–éœ€è¦å†æ–Ÿé…Œæ–Ÿé…Œ
    # print(decision)     # æµ‹è¯•
    return decision.f10, decision.f01   # å› ä¸ºæ˜¯äºŒä½å¹³é¢ï¼Œåˆ™é€‰æ‹©å¹‚ä¸º1çš„æ•°æ®ï¼Œå³x1,x2

def draw_boundary(power, l):
    """
    :param power: polynomial power for mapped feature
    :param l: å¸¸æ•°ï¼Œä½œä¸ºÎ»å€¼
    :return: å›¾åƒ
    """
    density = 1000
    threshold = 2 * 10 ** -3
    final_theta = regularized_logistic_regression(power, l)
    x, y = find_decision_boundary(density, power, final_theta, threshold)
    df = pd.read_csv('ex2data2.txt', names=['test1', 'test2', 'accepted'])
    sns.lmplot('test1', 'test2', hue='accepted', data=df, height=8, fit_reg=False, scatter_kws={"s": 40})
    plt.scatter(x, y, c='r', s=8)      # ç”»å‡ºæ•£ç‚¹å›¾ï¼Œçº¢è‰²
    plt.title('Decision boundary')
    plt.show()

draw_boundary(power=6, l=1)
# draw_boundary(power=6, l=0)     # è¿‡æ‹Ÿåˆ
# draw_boundary(power=6, l=100)      # lè¿‡å¤§ï¼Œæ¬ æ‹Ÿåˆæ•ˆæœ
